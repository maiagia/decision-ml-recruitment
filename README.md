
# ğŸš€ Datathon PÃ³s-Tech: Machine Learning Engineering - Recrutamento com IA

Este repositÃ³rio contÃ©m a soluÃ§Ã£o de **ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga)** desenvolvida para o desafio do Datathon do curso PÃ³s-Tech FIAP. A proposta Ã© usar IA para otimizar o processo de recrutamento da empresa fictÃ­cia **Decision**, que atua na Ã¡rea de bodyshop de TI.

---

## ğŸ§  Objetivo do Projeto

A empresa enfrenta dificuldades em encontrar e engajar candidatos ideais para vagas de TI. O desafio Ã© estruturar um pipeline que consolide os dados de candidatos, vagas e prospecÃ§Ãµes, gerando uma base unificada e limpa para alimentar modelos preditivos de "match".

---

## ğŸ“‚ Base de Dados

A base Ã© composta por trÃªs arquivos JSON:

- `vagas.json`: informaÃ§Ãµes de vagas, clientes, localizaÃ§Ã£o, requisitos.
- `applicants.json`: dados dos candidatos como nome, formaÃ§Ã£o, idiomas, currÃ­culo.
- `prospects.json`: histÃ³rico de encaminhamentos por vaga.

---

## âš™ï¸ Pipeline ETL

### âœ… ExtraÃ§Ã£o

Os arquivos JSON sÃ£o lidos da pasta `data/`:

```python
with open("../data/vagas.json") as f:
    jobs_data = json.load(f)
```

---

### ğŸ”„ TransformaÃ§Ã£o

O processo consolida e padroniza os dados:

- **NormalizaÃ§Ã£o de nÃ­veis** (idiomas e formaÃ§Ã£o)
- **Limpeza de textos** (currÃ­culo e requisitos)
- **CriaÃ§Ã£o do campo-alvo `match`**: `1` se o candidato foi contratado, `0` caso contrÃ¡rio.
- **Tratamento de campos ausentes com valores padrÃ£o**

Exemplo de limpeza de requisitos:

```python
requisitos = f"{tecnicos} {atividades}"
requisitos = limpar_texto(requisitos)
```

---

### ğŸ’¾ Carga

O `DataFrame` final Ã© salvo em:

```python
../output/dataset_unificado.csv
```

E pode ser visualizado com:

```python
df.head(100)
```

---

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ data/               # Arquivos JSON de entrada
â”‚   â”œâ”€â”€ output/             # Arquivo CSV final consolidado
â”‚   â”œâ”€â”€ notebooks/          # AnÃ¡lises e testes manuais
â”‚   â”œâ”€â”€ etl_pipeline.py     # LÃ³gica principal do ETL
â”‚   â””â”€â”€ requirements.txt    # Bibliotecas usadas
```

---

## âœ… Como Executar

1. Clone este repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/api-etl-pipeline.git
cd api-etl-pipeline/etl
```

2. Ative o ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate.bat  # Windows
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Execute o script:
```bash
python etl_pipeline.py
```

O arquivo `dataset_unificado.csv` serÃ¡ gerado na pasta `output/`.

---
